


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Contrib &mdash; Catalyst 22.02rc0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Core" href="core.html" />
    <link rel="prev" title="Callbacks" href="callbacks.html" /> 

  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167094275-1"></script>
  <script src="../_static/js/googleanalytics.min.js"></script>
  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://catalyst-team.com/" aria-label="Catalyst"></a>

      <div class="main-menu">
        <ul>

          <li>
            <a href="https://github.com/catalyst-team/catalyst">Catalyst</a>
          </li>
          <li>
            <a href="https://github.com/catalyst-team/codestyle">Codestyle</a>
          </li>
          <li>
            <a href="https://github.com/catalyst-team/dl-course">Course</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="catalyst-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="catalyst-left-menu" id="catalyst-left-menu">
      <div class="catalyst-side-scroll">
        <div class="catalyst-menu catalyst-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="catalyst-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href="https://catalyst-team.github.io/catalyst/versions.html">22.02</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Catalyst</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/quickstart.html">Quickstart 101</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/catalyst-team/catalyst#minimal-examples">Minimal examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&amp;sk=885b4409aecab505db0a63b06f19dcef">Catalyst — Accelerated Deep Learning R&amp;D</a></li>
</ul>
<p><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/ddp.html">Distributed training tutorial</a></li>
</ul>
<p><span class="caption-text">Core</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../core/callback.html">Callback</a></li>
<li class="toctree-l1"><a class="reference internal" href="../core/engine.html">Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../core/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../core/metric.html">Metric</a></li>
<li class="toctree-l1"><a class="reference internal" href="../core/runner.html">Runner</a></li>
</ul>
<p><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/intro.html">How to make X?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/checkpointing.html">Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/dataflow.html">Dataflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/dp.html">Dataparallel training (cpu, single/multi-gpu)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/ddp.html">Distributed training (multi-gpu, multi-node)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/early_stopping.html">Early stopping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/engines.html">Engines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/mixed_precision.html">Mixed precision training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/multi_components.html">Multiple components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/multi_keys.html">Multiple input and output keys</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/optuna.html">Optuna integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/settings.html">Settings</a></li>
</ul>
<p><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="callbacks.html">Callbacks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Contrib</a></li>
<li class="toctree-l1"><a class="reference internal" href="core.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="engines.html">Engines</a></li>
<li class="toctree-l1"><a class="reference internal" href="loggers.html">Loggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="runners.html">Runners</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utils</a></li>
</ul>
<p><span class="caption-text">Contribution guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/catalyst-team/catalyst/blob/master/CONTRIBUTING.md">How to start</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/catalyst-team/codestyle">Codestyle</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/catalyst-team/catalyst#acknowledgments">Acknowledgments</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="catalyst-container">
      <div class="catalyst-page-level-bar" id="catalyst-page-level-bar">
        <div class="catalyst-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="catalyst-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Contrib</li>
    
    
      <li class="catalyst-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/catalyst-team/catalyst/blob/master/docs/api/contrib.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="catalyst-shortcuts-wrapper" id="catalyst-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="catalyst-content-wrap" class="catalyst-content-wrap">
        <div class="catalyst-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="catalyst-article" class="catalyst-article">
              
  <section id="contrib">
<h1>Contrib<a class="headerlink" href="#contrib" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line">Note: under development, best contrib modules will be placed here with docs and examples.</div>
<div class="line">If you would like to see your contribution here - please open a Pull Request or write us on <a class="reference external" href="https://join.slack.com/t/catalyst-team-devs/shared_invite/zt-d9miirnn-z86oKDzFMKlMG4fgFdZafw">slack</a>.</div>
<div class="line"><br /></div>
</div>
<p>Catalyst contrib modules are supported in the code-as-a-documentation format.
If you are interested in the details - please, follow the code of the implementation.
If you are interested in contributing to the library - feel free to open a pull request.
For more information, please follow the <a class="reference external" href="https://github.com/catalyst-team/catalyst/tree/master/catalyst/contrib">code for contrib-based extensions</a>.</p>
<section id="module-catalyst.contrib.data">
<span id="data"></span><h2>Data<a class="headerlink" href="#module-catalyst.contrib.data" title="Permalink to this headline">¶</a></h2>
<section id="inbatchsamplers">
<h3>InBatchSamplers<a class="headerlink" href="#inbatchsamplers" title="Permalink to this headline">¶</a></h3>
<section id="inbatchtripletssampler">
<h4>InBatchTripletsSampler<a class="headerlink" href="#inbatchtripletssampler" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.sampler_inbatch.</code><code class="sig-name descname">InBatchTripletsSampler</code><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#InBatchTripletsSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.data.sampler_inbatch.IInbatchTripletSampler</span></code></p>
<p>Base class for a triplets samplers.
We expect that the child instances of this class
will be used to forming triplets inside the batches.
(Note. It is assumed that set of output features is a
subset of samples features inside the batch.)
The batches must contain at least 2 samples for
each class and at least 2 different classes,
such behaviour can be garantee via using
catalyst.data.sampler.BatchBalanceClassSampler</p>
<p>But you are not limited to using it in any other way.</p>
<dl class="method">
<dt id="catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler.sample">
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param">features: torch.Tensor, labels: Union[List[int], torch.Tensor]</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#InBatchTripletsSampler.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler.sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features</strong> – has the shape of [batch_size, feature_size]</p></li>
<li><p><strong>labels</strong> – labels of the samples in the batch</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(anchor, positive, negative)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>the batch of the triplets in the order below</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="alltripletssampler">
<h4>AllTripletsSampler<a class="headerlink" href="#alltripletssampler" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.sampler_inbatch.AllTripletsSampler">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.sampler_inbatch.</code><code class="sig-name descname">AllTripletsSampler</code><span class="sig-paren">(</span><em class="sig-param">max_output_triplets: int = 9223372036854775807</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#AllTripletsSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.AllTripletsSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler" title="catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler"><code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler</span></code></a></p>
<p>This sampler selects all the possible triplets for the given labels</p>
<dl class="method">
<dt id="catalyst.contrib.data.sampler_inbatch.AllTripletsSampler.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">max_output_triplets: int = 9223372036854775807</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#AllTripletsSampler.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.AllTripletsSampler.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>max_output_triplets</strong> – with the strategy of choosing all
the triplets, their number in the batch can be very large,
because of it we can sample only random part of them,
determined by this parameter.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="hardtripletssampler">
<h4>HardTripletsSampler<a class="headerlink" href="#hardtripletssampler" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.sampler_inbatch.HardTripletsSampler">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.sampler_inbatch.</code><code class="sig-name descname">HardTripletsSampler</code><span class="sig-paren">(</span><em class="sig-param">norm_required: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#HardTripletsSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.HardTripletsSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler" title="catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler"><code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.data.sampler_inbatch.InBatchTripletsSampler</span></code></a></p>
<p>This sampler selects hardest triplets based on distances between features:
the hardest positive sample has the maximal distance to the anchor sample,
the hardest negative sample has the minimal distance to the anchor sample.</p>
<p>Note that a typical triplet loss chart is as follows:
1. Falling: loss decreases to a value equal to the margin.
2. Long plato: the loss oscillates near the margin.
3. Falling: loss decreases to zero.</p>
<dl class="method">
<dt id="catalyst.contrib.data.sampler_inbatch.HardTripletsSampler.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">norm_required: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#HardTripletsSampler.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.HardTripletsSampler.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>norm_required</strong> – set True if features normalisation is needed</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="hardclustersampler">
<h4>HardClusterSampler<a class="headerlink" href="#hardclustersampler" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.sampler_inbatch.HardClusterSampler">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.sampler_inbatch.</code><code class="sig-name descname">HardClusterSampler</code><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#HardClusterSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.HardClusterSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.data.sampler_inbatch.IInbatchTripletSampler</span></code></p>
<p>This sampler selects hardest triplets based on distance to mean vectors:
anchor is a mean vector of features of i-th class in the batch,
the hardest positive sample is the most distant from anchor sample of
anchor’s class, the hardest negative sample is the closest mean vector
of another classes.</p>
<p>The batch must contain k samples for p classes in it (k &gt; 1, p &gt; 1).</p>
<dl class="method">
<dt id="catalyst.contrib.data.sampler_inbatch.HardClusterSampler.sample">
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param">features: torch.Tensor, labels: Union[List[int], torch.Tensor]</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/catalyst/contrib/data/sampler_inbatch.html#HardClusterSampler.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler_inbatch.HardClusterSampler.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>This method samples the hardest triplets in the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features</strong> – tensor of shape (batch_size; embed_dim) that contains
k samples for each of p classes</p></li>
<li><p><strong>labels</strong> – labels of the batch, list or tensor of size (batch_size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>p triplets of (mean_vector, positive, negative_mean_vector)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="samplers">
<h3>Samplers<a class="headerlink" href="#samplers" title="Permalink to this headline">¶</a></h3>
<section id="balancebatchsampler">
<h4>BalanceBatchSampler<a class="headerlink" href="#balancebatchsampler" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.sampler.BalanceBatchSampler">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.sampler.</code><code class="sig-name descname">BalanceBatchSampler</code><span class="sig-paren">(</span><em class="sig-param">labels: Union[List[int], numpy.ndarray], p: int, k: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler.html#BalanceBatchSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler.BalanceBatchSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.sampler.Sampler</span></code></p>
<p>This kind of sampler can be used for both metric learning and
classification task.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Deprecated realization, used for backward compatibility.
Please use <cite>BatchBalanceClassSampler</cite> instead.</p>
</div>
<p>Sampler with the given strategy for the C unique classes dataset:
- Selection P of C classes for the 1st batch
- Selection K instances for each class for the 1st batch
- Selection P of C - P remaining classes for 2nd batch
- Selection K instances for each class for the 2nd batch
- …
The epoch ends when there are no classes left.
So, the batch sise is P * K except the last one.</p>
<p>Thus, in each epoch, all the classes will be selected once, but this
does not mean that all the instances will be selected during the epoch.</p>
<p>One of the purposes of this sampler is to be used for
forming triplets and pos/neg pairs inside the batch.
To guarante existance of these pairs in the batch,
P and K should be &gt; 1. (1)</p>
<p>Behavior in corner cases:
- If a class does not contain K instances,
a choice will be made with repetition.
- If C % P == 1 then one of the classes should be dropped
otherwise statement (1) will not be met.</p>
<p>This type of sampling can be found in the classical paper of Person Re-Id,
where P equals 32 and K equals 4:
<a class="reference external" href="https://arxiv.org/abs/1703.07737">In Defense of the Triplet Loss for Person Re-Identification</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels</strong> – list of classes labeles for each elem in the dataset</p></li>
<li><p><strong>p</strong> – number of classes in a batch, should be &gt; 1</p></li>
<li><p><strong>k</strong> – number of instances of each class in a batch, should be &gt; 1</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="catalyst.contrib.data.sampler.BalanceBatchSampler.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">labels: Union[List[int], numpy.ndarray], p: int, k: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler.html#BalanceBatchSampler.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler.BalanceBatchSampler.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Sampler initialisation.</p>
</dd></dl>

<dl class="method">
<dt id="catalyst.contrib.data.sampler.BalanceBatchSampler.batch_size">
<em class="property">property </em><code class="sig-name descname">batch_size</code><a class="headerlink" href="#catalyst.contrib.data.sampler.BalanceBatchSampler.batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns:
this value should be used in DataLoader as batch size</p>
</dd></dl>

<dl class="method">
<dt id="catalyst.contrib.data.sampler.BalanceBatchSampler.batches_in_epoch">
<em class="property">property </em><code class="sig-name descname">batches_in_epoch</code><a class="headerlink" href="#catalyst.contrib.data.sampler.BalanceBatchSampler.batches_in_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns:
number of batches in an epoch</p>
</dd></dl>

</dd></dl>

</section>
<section id="dynamicbalanceclasssampler">
<h4>DynamicBalanceClassSampler<a class="headerlink" href="#dynamicbalanceclasssampler" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.sampler.DynamicBalanceClassSampler">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.sampler.</code><code class="sig-name descname">DynamicBalanceClassSampler</code><span class="sig-paren">(</span><em class="sig-param">labels: List[Union[str, int]], exp_lambda: float = 0.9, start_epoch: int = 0, max_d: Optional[int] = None, mode: Union[str, int] = 'downsampling', ignore_warning: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler.html#DynamicBalanceClassSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler.DynamicBalanceClassSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.sampler.Sampler</span></code></p>
<p>This kind of sampler can be used for classification tasks with significant
class imbalance.</p>
<p>The idea of this sampler that we start with the original class distribution
and gradually move to uniform class distribution like with downsampling.</p>
<p>Let’s define :math: D_i = #C_i/ #C_min where :math: #C_i is a size of class
i and :math: #C_min is a size of the rarest class, so :math: D_i define
class distribution. Also define :math: g(n_epoch) is a exponential
scheduler. On each epoch current :math: D_i  calculated as
:math: current D_i  = D_i ^ g(n_epoch),
after this data samples according this distribution.</p>
<p class="rubric">Notes</p>
<p>In the end of the training, epochs will contain only
min_size_class * n_classes examples. So, possible it will not
necessary to do validation on each epoch. For this reason use
ControlFlowCallback.</p>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">catalyst.data</span> <span class="kn">import</span> <span class="n">DynamicBalanceClassSampler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">data</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">DynamicBalanceClassSampler</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b_features</span><span class="p">,</span> <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span>
</pre></div>
</div>
<p>Sampler was inspired by <a class="reference external" href="https://arxiv.org/abs/1901.06783">https://arxiv.org/abs/1901.06783</a></p>
<dl class="method">
<dt id="catalyst.contrib.data.sampler.DynamicBalanceClassSampler.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">labels: List[Union[str, int]], exp_lambda: float = 0.9, start_epoch: int = 0, max_d: Optional[int] = None, mode: Union[str, int] = 'downsampling', ignore_warning: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/sampler.html#DynamicBalanceClassSampler.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.sampler.DynamicBalanceClassSampler.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels</strong> – list of labels for each elem in the dataset</p></li>
<li><p><strong>exp_lambda</strong> – exponent figure for schedule</p></li>
<li><p><strong>start_epoch</strong> – start epoch number, can be useful for multistage</p></li>
<li><p><strong>experiments</strong> – </p></li>
<li><p><strong>max_d</strong> – if not None, limit on the difference between the most</p></li>
<li><p><strong>and the rarest classes</strong><strong>, </strong><strong>heuristic</strong> (<em>frequent</em>) – </p></li>
<li><p><strong>mode</strong> – number of samples per class in the end of training. Must be</p></li>
<li><p><strong>or number. Before change it</strong><strong>, </strong><strong>make sure that you</strong> (<em>&quot;downsampling&quot;</em>) – </p></li>
<li><p><strong>how does it work</strong> (<em>understand</em>) – </p></li>
<li><p><strong>ignore_warning</strong> – ignore warning about min class size</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="transforms">
<h3>Transforms<a class="headerlink" href="#transforms" title="Permalink to this headline">¶</a></h3>
<section id="compose">
<h4>Compose<a class="headerlink" href="#compose" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.transforms.Compose">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.transforms.</code><code class="sig-name descname">Compose</code><span class="sig-paren">(</span><em class="sig-param">transforms</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/transforms.html#Compose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.transforms.Compose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Composes several transforms together.</p>
<dl class="method">
<dt id="catalyst.contrib.data.transforms.Compose.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">transforms</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/transforms.html#Compose.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.transforms.Compose.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>transforms</strong> – list of transforms to compose.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Compose</span><span class="p">([</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">Normalize</span><span class="p">()])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="imagetotensor">
<h4>ImageToTensor<a class="headerlink" href="#imagetotensor" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.transforms.ImageToTensor">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.transforms.</code><code class="sig-name descname">ImageToTensor</code><a class="reference internal" href="../_modules/catalyst/contrib/data/transforms.html#ImageToTensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.transforms.ImageToTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Convert a <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> to tensor.
Converts numpy.ndarray (H x W x C) in the range [0, 255] to a
torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]
if the numpy.ndarray has dtype = np.uint8
In the other cases, tensors are returned without scaling.</p>
<dl class="method">
<dt id="catalyst.contrib.data.transforms.ImageToTensor.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.data.transforms.ImageToTensor.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

</dd></dl>

</section>
<section id="normalizeimage">
<h4>NormalizeImage<a class="headerlink" href="#normalizeimage" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="catalyst.contrib.data.transforms.NormalizeImage">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.data.transforms.</code><code class="sig-name descname">NormalizeImage</code><span class="sig-paren">(</span><em class="sig-param">mean</em>, <em class="sig-param">std</em>, <em class="sig-param">inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/transforms.html#NormalizeImage"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.transforms.NormalizeImage" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Normalize a tensor image with mean and standard deviation.</p>
<p>Given mean: <code class="docutils literal notranslate"><span class="pre">(mean[1],...,mean[n])</span></code> and std: <code class="docutils literal notranslate"><span class="pre">(std[1],..,std[n])</span></code>
for <code class="docutils literal notranslate"><span class="pre">n</span></code> channels, this transform will normalize each channel of the input
<code class="docutils literal notranslate"><span class="pre">torch.*Tensor</span></code> i.e.,
<code class="docutils literal notranslate"><span class="pre">output[channel]</span> <span class="pre">=</span> <span class="pre">(input[channel]</span> <span class="pre">-</span> <span class="pre">mean[channel])</span> <span class="pre">/</span> <span class="pre">std[channel]</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>This transform acts out of place, i.e.,</dt><dd><p>it does not mutate the input tensor.</p>
</dd>
</dl>
</div>
<dl class="method">
<dt id="catalyst.contrib.data.transforms.NormalizeImage.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">mean</em>, <em class="sig-param">std</em>, <em class="sig-param">inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/data/transforms.html#NormalizeImage.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.data.transforms.NormalizeImage.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mean</strong> – Sequence of means for each channel.</p></li>
<li><p><strong>std</strong> – Sequence of standard deviations for each channel.</p></li>
<li><p><strong>inplace</strong> (<em>bool</em><em>,</em><em>optional</em>) – Bool to make this operation in-place.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
</section>
<section id="module-catalyst.contrib.datasets">
<span id="datasets"></span><h2>Datasets<a class="headerlink" href="#module-catalyst.contrib.datasets" title="Permalink to this headline">¶</a></h2>
<section id="cifar10">
<h3>CIFAR10<a class="headerlink" href="#cifar10" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.cifar.CIFAR10">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.cifar.</code><code class="sig-name descname">CIFAR10</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">transform: Optional[Callable] = None</em>, <em class="sig-param">target_transform: Optional[Callable] = None</em>, <em class="sig-param">download: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/cifar.html#CIFAR10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.cifar.CIFAR10" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.cifar.VisionDataset</span></code></p>
<p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">cifar-10-batches-py</span></code> exists or will be saved to if download is set to True.</p></li>
<li><p><strong>train</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, creates dataset from training set, otherwise
creates from test set.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>download</strong> (<em>bool</em><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="catalyst.contrib.datasets.cifar.CIFAR10.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">transform: Optional[Callable] = None</em>, <em class="sig-param">target_transform: Optional[Callable] = None</em>, <em class="sig-param">download: bool = False</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../_modules/catalyst/contrib/datasets/cifar.html#CIFAR10.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.cifar.CIFAR10.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="cifar100">
<h3>CIFAR100<a class="headerlink" href="#cifar100" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.cifar.CIFAR100">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.cifar.</code><code class="sig-name descname">CIFAR100</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">transform: Optional[Callable] = None</em>, <em class="sig-param">target_transform: Optional[Callable] = None</em>, <em class="sig-param">download: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/cifar.html#CIFAR100"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.cifar.CIFAR100" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#catalyst.contrib.datasets.cifar.CIFAR10" title="catalyst.contrib.datasets.cifar.CIFAR10"><code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.cifar.CIFAR10</span></code></a></p>
<p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR100</a> Dataset.</p>
<p>This is a subclass of the <cite>CIFAR10</cite> Dataset.</p>
<dl class="method">
<dt id="catalyst.contrib.datasets.cifar.CIFAR100.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">transform: Optional[Callable] = None</em>, <em class="sig-param">target_transform: Optional[Callable] = None</em>, <em class="sig-param">download: bool = False</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#catalyst.contrib.datasets.cifar.CIFAR100.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="imagenette">
<h3>Imagenette<a class="headerlink" href="#imagenette" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.imagenette.Imagenette">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.imagenette.</code><code class="sig-name descname">Imagenette</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagenette.html#Imagenette"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.imagenette.Imagenette" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#imagenette-1">Imagenette</a> Dataset.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.imagenette.Imagenette.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.imagenette.Imagenette.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="imagenette160">
<h3>Imagenette160<a class="headerlink" href="#imagenette160" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.imagenette.Imagenette160">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.imagenette.</code><code class="sig-name descname">Imagenette160</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagenette.html#Imagenette160"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.imagenette.Imagenette160" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#imagenette-1">Imagenette</a> Dataset
with images resized so that the shortest size is 160 px.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.imagenette.Imagenette160.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.imagenette.Imagenette160.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="imagenette320">
<h3>Imagenette320<a class="headerlink" href="#imagenette320" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.imagenette.Imagenette320">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.imagenette.</code><code class="sig-name descname">Imagenette320</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagenette.html#Imagenette320"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.imagenette.Imagenette320" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#imagenette-1">Imagenette</a> Dataset
with images resized so that the shortest size is 320 px.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.imagenette.Imagenette320.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.imagenette.Imagenette320.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="imagewang">
<h3>Imagewang<a class="headerlink" href="#imagewang" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.imagewang.Imagewang">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.imagewang.</code><code class="sig-name descname">Imagewang</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagewang.html#Imagewang"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.imagewang.Imagewang" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#image%E7%BD%91">Imagewang</a> Dataset.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.imagewang.Imagewang.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.imagewang.Imagewang.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="imagewang160">
<h3>Imagewang160<a class="headerlink" href="#imagewang160" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.imagewang.Imagewang160">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.imagewang.</code><code class="sig-name descname">Imagewang160</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagewang.html#Imagewang160"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.imagewang.Imagewang160" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#image%E7%BD%91">Imagewang</a> Dataset
with images resized so that the shortest size is 160 px.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.imagewang.Imagewang160.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.imagewang.Imagewang160.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="imagewang320">
<h3>Imagewang320<a class="headerlink" href="#imagewang320" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.imagewang.Imagewang320">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.imagewang.</code><code class="sig-name descname">Imagewang320</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagewang.html#Imagewang320"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.imagewang.Imagewang320" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#image%E7%BD%91">Imagewang</a> Dataset
with images resized so that the shortest size is 320 px.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.imagewang.Imagewang320.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.imagewang.Imagewang320.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="imagewoof">
<h3>Imagewoof<a class="headerlink" href="#imagewoof" title="Permalink to this headline">¶</a></h3>
<dl class="attribute">
<dt id="catalyst.contrib.datasets.imagewoof">
<code class="sig-prename descclassname">catalyst.contrib.datasets.</code><code class="sig-name descname">imagewoof</code><a class="headerlink" href="#catalyst.contrib.datasets.imagewoof" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#catalyst.contrib.datasets.imagewoof" title="catalyst.contrib.datasets.imagewoof"><code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.imagewoof</span></code></a></p>
</dd></dl>

</section>
<section id="imagewoof160">
<h3>Imagewoof160<a class="headerlink" href="#imagewoof160" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.Imagewoof160">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.</code><code class="sig-name descname">Imagewoof160</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagewoof.html#Imagewoof160"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.Imagewoof160" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#imagewoof">Imagewoof</a> Dataset
with images resized so that the shortest size is 160 px.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.Imagewoof160.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.Imagewoof160.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="imagewoof320">
<h3>Imagewoof320<a class="headerlink" href="#imagewoof320" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.Imagewoof320">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.</code><code class="sig-name descname">Imagewoof320</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/imagewoof.html#Imagewoof320"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.Imagewoof320" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.datasets.misc_cv.ImageClassificationDataset</span></code></p>
<p><a class="reference external" href="https://github.com/fastai/imagenette#imagewoof">Imagewoof</a> Dataset
with images resized so that the shortest size is 320 px.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[cv] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.Imagewoof320.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.datasets.Imagewoof320.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor method for the <code class="docutils literal notranslate"><span class="pre">ImageClassificationDataset</span></code> class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – root directory of dataset</p></li>
<li><p><strong>train</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates dataset from <code class="docutils literal notranslate"><span class="pre">train/</span></code>
subfolder, otherwise from <code class="docutils literal notranslate"><span class="pre">val/</span></code></p></li>
<li><p><strong>download</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again</p></li>
<li><p><strong>**kwargs</strong> – Keyword-arguments passed to <code class="docutils literal notranslate"><span class="pre">super().__init__</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="mnist">
<h3>MNIST<a class="headerlink" href="#mnist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.mnist.MNIST">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.mnist.</code><code class="sig-name descname">MNIST</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = True</em>, <em class="sig-param">normalize: tuple = (0.1307</em>, <em class="sig-param">0.3081)</em>, <em class="sig-param">numpy: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/mnist.html#MNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.mnist.MNIST" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p><a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> Dataset for testing purposes.</p>
<blockquote>
<div><dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>root: Root directory of dataset where</dt><dd><p><code class="docutils literal notranslate"><span class="pre">MNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">MNIST/processed/test.pt</span></code> exist.</p>
</dd>
<dt>train (bool, optional): If True, creates dataset from</dt><dd><p><code class="docutils literal notranslate"><span class="pre">training.pt</span></code>, otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</p>
</dd>
<dt>download (bool, optional): If true, downloads the dataset from</dt><dd><p>the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again.</p>
</dd>
<dt>normalize (tuple, optional): mean and std</dt><dd><p>for the MNIST dataset normalization.</p>
</dd>
<dt>numpy (bool, optional): boolean flag to return an np.ndarray,</dt><dd><p>rather than torch.tensor (default: False).</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> – If <code class="docutils literal notranslate"><span class="pre">download</span> <span class="pre">is</span> <span class="pre">False</span></code> and the dataset not found.</p>
</dd>
</dl>
<dl class="method">
<dt id="catalyst.contrib.datasets.mnist.MNIST.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root: str</em>, <em class="sig-param">train: bool = True</em>, <em class="sig-param">download: bool = True</em>, <em class="sig-param">normalize: tuple = (0.1307</em>, <em class="sig-param">0.3081)</em>, <em class="sig-param">numpy: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/mnist.html#MNIST.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.mnist.MNIST.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Init.</p>
</dd></dl>

</dd></dl>

</section>
<section id="movielens">
<h3>MovieLens<a class="headerlink" href="#movielens" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.datasets.movielens.MovieLens">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.datasets.movielens.</code><code class="sig-name descname">MovieLens</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">download=False</em>, <em class="sig-param">min_rating=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/movielens.html#MovieLens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.movielens.MovieLens" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>MovieLens data sets were collected by the GroupLens Research Project
at the University of Minnesota.</p>
<p>This data set consists of:
* 100,000 ratings (1-5) from 943 users on 1682 movies.
* Each user has rated at least 20 movies.
* Simple demographic info for the users
(age, gender, occupation, zip)</p>
<p>The data was collected through the MovieLens web site
(movielens.umn.edu) during the seven-month period from September 19th,
1997 through April 22nd, 1998. This data has been cleaned up - users
who had less than 20 ratings or did not have complete demographic
information were removed from this data set. Detailed descriptions of
the data file can be found at the end of this file.</p>
<p>Neither the University of Minnesota nor any of the researchers
involved can guarantee the correctness of the data, its suitability
for any particular purpose, or the validity of results based on the
use of the data set.  The data set may be used for any research
purposes under the following conditions:
* The user may not state or imply any endorsement from the
University of Minnesota or the GroupLens Research Group.
* The user must acknowledge the use of the data set in
publications resulting from the use of the data set
(see below for citation information).
* The user may not redistribute the data without separate
permission.
* The user may not use this information for any commercial or
revenue-bearing purposes without first obtaining permission
from a faculty member of the GroupLens Research Project at the
University of Minnesota.</p>
<p>If you have any further questions or comments, please contact GroupLens
&lt;<a class="reference external" href="mailto:grouplens-info&#37;&#52;&#48;cs&#46;umn&#46;edu">grouplens-info<span>&#64;</span>cs<span>&#46;</span>umn<span>&#46;</span>edu</a>&gt;.
<a class="reference external" href="http://files.grouplens.org/datasets/movielens/ml-100k-README.txt">http://files.grouplens.org/datasets/movielens/ml-100k-README.txt</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>catalyst[ml] required for this dataset.</p>
</div>
<dl class="method">
<dt id="catalyst.contrib.datasets.movielens.MovieLens.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">download=False</em>, <em class="sig-param">min_rating=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/datasets/movielens.html#MovieLens.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.datasets.movielens.MovieLens.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where
<code class="docutils literal notranslate"><span class="pre">MovieLens/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">MovieLens/processed/test.pt</span></code> exist.</p></li>
<li><p><strong>train</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, creates dataset from
<code class="docutils literal notranslate"><span class="pre">training.pt</span></code>, otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</p></li>
<li><p><strong>download</strong> (<em>bool</em><em>, </em><em>optional</em>) – If true, downloads the dataset from
the internet and puts it in root directory. If dataset
is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>min_rating</strong> (<em>float</em><em>, </em><em>optional</em>) – Minimum rating to include in
the interaction matrix</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – If <code class="docutils literal notranslate"><span class="pre">download</span> <span class="pre">is</span> <span class="pre">False</span></code> and the dataset not found.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="layers">
<h2>Layers<a class="headerlink" href="#layers" title="Permalink to this headline">¶</a></h2>
<section id="adacos">
<h3>AdaCos<a class="headerlink" href="#adacos" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.cosface.AdaCos">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.cosface.</code><code class="sig-name descname">AdaCos</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em>, <em class="sig-param">dynamical_s: bool = True</em>, <em class="sig-param">eps: float = 1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/cosface.html#AdaCos"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.cosface.AdaCos" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of
<a class="reference external" href="https://arxiv.org/abs/1905.00292">AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
<li><p><strong>dynamical_s</strong> – option to use dynamical scale parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code> then will be used initial scale.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>eps</strong> – operation accuracy.
Default: <code class="docutils literal notranslate"><span class="pre">1e-6</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">AdaCos</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrosEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.cosface.AdaCos.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">target: torch.LongTensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/cosface.html#AdaCos.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.cosface.AdaCos.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p></li>
<li><p><strong>target</strong> – target classes,
expected shapes <code class="docutils literal notranslate"><span class="pre">B</span></code> where
<code class="docutils literal notranslate"><span class="pre">B</span></code> is batch dimension.
If <cite>None</cite> then will be returned
projection on centroids.
Default is <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes
(out_features).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="amsoftmax">
<h3>AMSoftmax<a class="headerlink" href="#amsoftmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.amsoftmax.AMSoftmax">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.amsoftmax.</code><code class="sig-name descname">AMSoftmax</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em>, <em class="sig-param">s: float = 64.0</em>, <em class="sig-param">m: float = 0.5</em>, <em class="sig-param">eps: float = 1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/amsoftmax.html#AMSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.amsoftmax.AMSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of
<a class="reference external" href="https://arxiv.org/pdf/1801.05599.pdf">AMSoftmax: Additive Margin Softmax for Face Verification</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
<li><p><strong>s</strong> – norm of input feature.
Default: <code class="docutils literal notranslate"><span class="pre">64.0</span></code>.</p></li>
<li><p><strong>m</strong> – margin.
Default: <code class="docutils literal notranslate"><span class="pre">0.5</span></code>.</p></li>
<li><p><strong>eps</strong> – operation accuracy.
Default: <code class="docutils literal notranslate"><span class="pre">1e-6</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">AMSoftmax</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">1.31</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.amsoftmax.AMSoftmax.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">target: torch.LongTensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/amsoftmax.html#AMSoftmax.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.amsoftmax.AMSoftmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p></li>
<li><p><strong>target</strong> – target classes,
expected shapes <code class="docutils literal notranslate"><span class="pre">B</span></code> where
<code class="docutils literal notranslate"><span class="pre">B</span></code> is batch dimension.
If <cite>None</cite> then will be returned
projection on centroids.
Default is <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes
(out_features).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="arcface">
<h3>ArcFace<a class="headerlink" href="#arcface" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.arcface.ArcFace">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.arcface.</code><code class="sig-name descname">ArcFace</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em>, <em class="sig-param">s: float = 64.0</em>, <em class="sig-param">m: float = 0.5</em>, <em class="sig-param">eps: float = 1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/arcface.html#ArcFace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.arcface.ArcFace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of
<a class="reference external" href="https://arxiv.org/abs/1801.07698v1">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
<li><p><strong>s</strong> – norm of input feature.
Default: <code class="docutils literal notranslate"><span class="pre">64.0</span></code>.</p></li>
<li><p><strong>m</strong> – margin.
Default: <code class="docutils literal notranslate"><span class="pre">0.5</span></code>.</p></li>
<li><p><strong>eps</strong> – operation accuracy.
Default: <code class="docutils literal notranslate"><span class="pre">1e-6</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">ArcFace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">1.31</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.arcface.ArcFace.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">target: torch.LongTensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/arcface.html#ArcFace.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.arcface.ArcFace.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p></li>
<li><p><strong>target</strong> – target classes,
expected shapes <code class="docutils literal notranslate"><span class="pre">B</span></code> where
<code class="docutils literal notranslate"><span class="pre">B</span></code> is batch dimension.
If <cite>None</cite> then will be returned
projection on centroids.
Default is <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes
(out_features).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="arcmarginproduct">
<h3>ArcMarginProduct<a class="headerlink" href="#arcmarginproduct" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.arcmargin.ArcMarginProduct">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.arcmargin.</code><code class="sig-name descname">ArcMarginProduct</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/arcmargin.html#ArcMarginProduct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.arcmargin.ArcMarginProduct" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of Arc Margin Product.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">ArcMarginProduct</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrosEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.arcmargin.ArcMarginProduct.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/arcmargin.html#ArcMarginProduct.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.arcmargin.ArcMarginProduct.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes
(out_features).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="cse">
<h3>cSE<a class="headerlink" href="#cse" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.se.cSE">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.se.</code><code class="sig-name descname">cSE</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em>, <em class="sig-param">r: int = 16</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/se.html#cSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.se.cSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The channel-wise SE (Squeeze and Excitation) block from the
<a class="reference external" href="https://arxiv.org/abs/1709.01507">Squeeze-and-Excitation Networks</a> paper.</p>
<p>Adapted from
<a class="reference external" href="https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939">https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939</a>
and
<a class="reference external" href="https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178">https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178</a></p>
<p>Shape:</p>
<ul class="simple">
<li><p>Input: (batch, channels, height, width)</p></li>
<li><p>Output: (batch, channels, height, width) (same shape as input)</p></li>
</ul>
<dl class="method">
<dt id="catalyst.contrib.layers.se.cSE.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em>, <em class="sig-param">r: int = 16</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/se.html#cSE.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.se.cSE.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> – The number of channels
in the feature map of the input.</p></li>
<li><p><strong>r</strong> – The reduction ratio of the intermediate channels.
Default: 16.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="cosface">
<h3>CosFace<a class="headerlink" href="#cosface" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.cosface.CosFace">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.cosface.</code><code class="sig-name descname">CosFace</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em>, <em class="sig-param">s: float = 64.0</em>, <em class="sig-param">m: float = 0.35</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/cosface.html#CosFace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.cosface.CosFace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of
<a class="reference external" href="https://arxiv.org/abs/1801.09414">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
<li><p><strong>s</strong> – norm of input feature.
Default: <code class="docutils literal notranslate"><span class="pre">64.0</span></code>.</p></li>
<li><p><strong>m</strong> – margin.
Default: <code class="docutils literal notranslate"><span class="pre">0.35</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">CosFaceLoss</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">1.31</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrosEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.cosface.CosFace.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">target: torch.LongTensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/cosface.html#CosFace.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.cosface.CosFace.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p></li>
<li><p><strong>target</strong> – target classes,
expected shapes <code class="docutils literal notranslate"><span class="pre">B</span></code> where
<code class="docutils literal notranslate"><span class="pre">B</span></code> is batch dimension.
If <cite>None</cite> then will be returned
projection on centroids.
Default is <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes
(out_features).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="curricularface">
<h3>CurricularFace<a class="headerlink" href="#curricularface" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.curricularface.CurricularFace">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.curricularface.</code><code class="sig-name descname">CurricularFace</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em>, <em class="sig-param">s: float = 64.0</em>, <em class="sig-param">m: float = 0.5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/curricularface.html#CurricularFace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.curricularface.CurricularFace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of
<a class="reference external" href="https://arxiv.org/abs/2004.00288">CurricularFace: Adaptive Curriculum Learning        Loss for Deep Face Recognition</a>.</p>
<p>Official <a class="reference external" href="https://github.com/HuangYG123/CurricularFace">pytorch implementation</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
<li><p><strong>s</strong> – norm of input feature.
Default: <code class="docutils literal notranslate"><span class="pre">64.0</span></code>.</p></li>
<li><p><strong>m</strong> – margin.
Default: <code class="docutils literal notranslate"><span class="pre">0.5</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">CurricularFace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">1.31</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrosEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.curricularface.CurricularFace.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">label: torch.LongTensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/curricularface.html#CurricularFace.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.curricularface.CurricularFace.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p></li>
<li><p><strong>label</strong> – target classes,
expected shapes <code class="docutils literal notranslate"><span class="pre">B</span></code> where
<code class="docutils literal notranslate"><span class="pre">B</span></code> is batch dimension.
If <cite>None</cite> then will be returned
projection on centroids.
Default is <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="factorizedlinear">
<h3>FactorizedLinear<a class="headerlink" href="#factorizedlinear" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.factorized.FactorizedLinear">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.factorized.</code><code class="sig-name descname">FactorizedLinear</code><span class="sig-paren">(</span><em class="sig-param">nn_linear: torch.nn.modules.linear.Linear</em>, <em class="sig-param">dim_ratio: Union[int</em>, <em class="sig-param">float] = 1.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/factorized.html#FactorizedLinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.factorized.FactorizedLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Factorized wrapper for <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nn_linear</strong> – torch <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> module</p></li>
<li><p><strong>dim_ratio</strong> – dimension ration to use after weights SVD</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="catalyst.contrib.layers.factorized.FactorizedLinear.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="../_modules/catalyst/contrib/layers/factorized.html#FactorizedLinear.extra_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.factorized.FactorizedLinear.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Extra representation log.</p>
</dd></dl>

<dl class="method">
<dt id="catalyst.contrib.layers.factorized.FactorizedLinear.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/factorized.html#FactorizedLinear.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.factorized.FactorizedLinear.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward call.</p>
</dd></dl>

</dd></dl>

</section>
<section id="scse">
<h3>scSE<a class="headerlink" href="#scse" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.se.scSE">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.se.</code><code class="sig-name descname">scSE</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em>, <em class="sig-param">r: int = 16</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/se.html#scSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.se.scSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The scSE (Concurrent Spatial and Channel Squeeze and Channel Excitation)
block from the <a class="reference external" href="https://arxiv.org/abs/1803.02579">Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’
in Fully Convolutional Networks</a> paper.</p>
<p>Adapted from
<a class="reference external" href="https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178">https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178</a></p>
<p>Shape:</p>
<ul class="simple">
<li><p>Input: (batch, channels, height, width)</p></li>
<li><p>Output: (batch, channels, height, width) (same shape as input)</p></li>
</ul>
<dl class="method">
<dt id="catalyst.contrib.layers.se.scSE.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em>, <em class="sig-param">r: int = 16</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/se.html#scSE.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.se.scSE.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> – The number of channels
in the feature map of the input.</p></li>
<li><p><strong>r</strong> – The reduction ratio of the intermediate channels.
Default: 16.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-catalyst.contrib.layers.softmax">
<span id="softmax"></span><h3>SoftMax<a class="headerlink" href="#module-catalyst.contrib.layers.softmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.softmax.SoftMax">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.softmax.</code><code class="sig-name descname">SoftMax</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">num_classes: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/softmax.html#SoftMax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.softmax.SoftMax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of
<a class="reference external" href="https://arxiv.org/abs/1712.10151">Significance of Softmax-based Features in Comparison to
Distance Metric Learning-based Features</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">SoftMax</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrosEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.softmax.SoftMax.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/softmax.html#SoftMax.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.softmax.SoftMax.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes
(out_features).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="sse">
<h3>sSE<a class="headerlink" href="#sse" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.se.sSE">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.se.</code><code class="sig-name descname">sSE</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/se.html#sSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.se.sSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The sSE (Channel Squeeze and Spatial Excitation) block from the
<a class="reference external" href="https://arxiv.org/abs/1803.02579">Concurrent Spatial and Channel ‘Squeeze &amp; Excitation’
in Fully Convolutional Networks</a> paper.</p>
<p>Adapted from
<a class="reference external" href="https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178">https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178</a></p>
<p>Shape:</p>
<ul class="simple">
<li><p>Input: (batch, channels, height, width)</p></li>
<li><p>Output: (batch, channels, height, width) (same shape as input)</p></li>
</ul>
<dl class="method">
<dt id="catalyst.contrib.layers.se.sSE.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/se.html#sSE.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.se.sSE.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>in_channels</strong> – The number of channels
in the feature map of the input.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="subcenterarcface">
<h3>SubCenterArcFace<a class="headerlink" href="#subcenterarcface" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.layers.arcface.SubCenterArcFace">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.layers.arcface.</code><code class="sig-name descname">SubCenterArcFace</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em>, <em class="sig-param">s: float = 64.0</em>, <em class="sig-param">m: float = 0.5</em>, <em class="sig-param">k: int = 3</em>, <em class="sig-param">eps: float = 1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/layers/arcface.html#SubCenterArcFace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.arcface.SubCenterArcFace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of
<a class="reference external" href="https://ibug.doc.ic.ac.uk/media/uploads/documents/eccv_1445.pdf">Sub-center ArcFace: Boosting Face Recognition
by Large-scale Noisy Web Faces</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample.</p></li>
<li><p><strong>out_features</strong> – size of each output sample.</p></li>
<li><p><strong>s</strong> – norm of input feature,
Default: <code class="docutils literal notranslate"><span class="pre">64.0</span></code>.</p></li>
<li><p><strong>m</strong> – margin.
Default: <code class="docutils literal notranslate"><span class="pre">0.5</span></code>.</p></li>
<li><p><strong>k</strong> – number of possible class centroids.
Default: <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – operation accuracy.
Default: <code class="docutils literal notranslate"><span class="pre">1e-6</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((batch, H_{in})\)</span> where
<span class="math notranslate nohighlight">\(H_{in} = in\_features\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((batch, H_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = out\_features\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">SubCenterArcFace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">1.31</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrosEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.layers.arcface.SubCenterArcFace.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">target: torch.LongTensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/layers/arcface.html#SubCenterArcFace.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.layers.arcface.SubCenterArcFace.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input features,
expected shapes <code class="docutils literal notranslate"><span class="pre">BxF</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span></code>
is batch dimension and <code class="docutils literal notranslate"><span class="pre">F</span></code> is an
input feature dimension.</p></li>
<li><p><strong>target</strong> – target classes,
expected shapes <code class="docutils literal notranslate"><span class="pre">B</span></code> where
<code class="docutils literal notranslate"><span class="pre">B</span></code> is batch dimension.
If <cite>None</cite> then will be returned
projection on centroids.
Default is <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor (logits) with shapes <code class="docutils literal notranslate"><span class="pre">BxC</span></code>
where <code class="docutils literal notranslate"><span class="pre">C</span></code> is a number of classes.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="losses">
<h2>Losses<a class="headerlink" href="#losses" title="Permalink to this headline">¶</a></h2>
<section id="adaptivehingeloss">
<h3>AdaptiveHingeLoss<a class="headerlink" href="#adaptivehingeloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.recsys.AdaptiveHingeLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.recsys.</code><code class="sig-name descname">AdaptiveHingeLoss</code><a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#AdaptiveHingeLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.AdaptiveHingeLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.losses.recsys.PairwiseLoss</span></code></p>
<p>Adaptive hinge loss function.</p>
<p>Takes a set of predictions for implicitly negative items, and selects those
that are highest, thus sampling those negatives that are closes to violating
the ranking implicit in the pattern of user interactions.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib.losses</span> <span class="kn">import</span> <span class="n">recsys</span>

<span class="n">pos_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">neg_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">recsys</span><span class="o">.</span><span class="n">AdaptiveHingeLoss</span><span class="p">()(</span><span class="n">pos_score</span><span class="p">,</span> <span class="n">neg_scores</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.recsys.AdaptiveHingeLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">positive_score: torch.Tensor</em>, <em class="sig-param">negative_scores: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#AdaptiveHingeLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.AdaptiveHingeLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation method for the adaptive hinge loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>positive_score</strong> – Tensor containing predictions for known positive items.</p></li>
<li><p><strong>negative_scores</strong> – Iterable of tensors containing predictions for sampled negative items.
More tensors increase the likelihood of finding ranking-violating pairs,
but risk overfitting.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>computed loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="barlowtwinsloss">
<h3>BarlowTwinsLoss<a class="headerlink" href="#barlowtwinsloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.contrastive.BarlowTwinsLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.contrastive.</code><code class="sig-name descname">BarlowTwinsLoss</code><span class="sig-paren">(</span><em class="sig-param">offdiag_lambda=1.0</em>, <em class="sig-param">eps=1e-12</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/contrastive.html#BarlowTwinsLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.contrastive.BarlowTwinsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The Contrastive embedding loss.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/2103.03230">Barlow Twins:
Self-Supervised Learning via Redundancy Reduction</a>.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib</span> <span class="kn">import</span> <span class="n">BarlowTwinsLoss</span>

<span class="n">embeddings_left</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">embeddings_right</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">BarlowTwinsLoss</span><span class="p">(</span><span class="n">offdiag_lambda</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">criterion</span><span class="p">(</span><span class="n">embeddings_left</span><span class="p">,</span> <span class="n">embeddings_right</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.contrastive.BarlowTwinsLoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">offdiag_lambda=1.0</em>, <em class="sig-param">eps=1e-12</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/contrastive.html#BarlowTwinsLoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.contrastive.BarlowTwinsLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>offdiag_lambda</strong> – trade-off parameter</p></li>
<li><p><strong>eps</strong> – shift for the varience (var + eps)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="bprloss">
<h3>BPRLoss<a class="headerlink" href="#bprloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.recsys.BPRLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.recsys.</code><code class="sig-name descname">BPRLoss</code><span class="sig-paren">(</span><em class="sig-param">gamma=1e-10</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#BPRLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.BPRLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.losses.recsys.PairwiseLoss</span></code></p>
<p>Bayesian Personalised Ranking loss function.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/pdf/1205.2618.pdf">BPRLoss: Bayesian Personalized Ranking from Implicit Feedback</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gamma</strong> (<em>float</em>) – Small value to avoid division by zero. Default: <code class="docutils literal notranslate"><span class="pre">1e-10</span></code>.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib.losses</span> <span class="kn">import</span> <span class="n">recsys</span>

<span class="n">pos_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">neg_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">recsys</span><span class="o">.</span><span class="n">BPRLoss</span><span class="p">()(</span><span class="n">pos_score</span><span class="p">,</span> <span class="n">neg_score</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.recsys.BPRLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">positive_score: torch.Tensor</em>, <em class="sig-param">negative_score: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#BPRLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.BPRLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation method for the BPR loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>positive_score</strong> – Tensor containing predictions for known positive items.</p></li>
<li><p><strong>negative_score</strong> – Tensor containing predictions for sampled negative items.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>computed loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="circleloss">
<h3>CircleLoss<a class="headerlink" href="#circleloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.circle.CircleLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.circle.</code><code class="sig-name descname">CircleLoss</code><span class="sig-paren">(</span><em class="sig-param">margin: float</em>, <em class="sig-param">gamma: float</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/circle.html#CircleLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.circle.CircleLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>CircleLoss from
<a class="reference external" href="https://arxiv.org/abs/2002.10857">Circle Loss: A Unified Perspective of Pair Similarity Optimization</a> paper.</p>
<p>Adapter from:
<a class="reference external" href="https://github.com/TinyZeaMays/CircleLoss">https://github.com/TinyZeaMays/CircleLoss</a></p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">catalyst.contrib.losses</span> <span class="kn">import</span> <span class="n">CircleLoss</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="n">CircleLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.circle.CircleLoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">margin: float</em>, <em class="sig-param">gamma: float</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../_modules/catalyst/contrib/losses/circle.html#CircleLoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.circle.CircleLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> – margin to use</p></li>
<li><p><strong>gamma</strong> – gamma to use</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="diceloss">
<h3>DiceLoss<a class="headerlink" href="#diceloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.dice.DiceLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.dice.</code><code class="sig-name descname">DiceLoss</code><span class="sig-paren">(</span><em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/dice.html#DiceLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.dice.DiceLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The Dice loss.
DiceLoss = 1 - dice score
dice score = 2 * intersection / (intersection + union)) =     = 2 * tp / (2 * tp + fp + fn)</p>
<dl class="method">
<dt id="catalyst.contrib.losses.dice.DiceLoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/dice.html#DiceLoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.dice.DiceLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>class_dim</strong> – indicates class dimention (K) for
<code class="docutils literal notranslate"><span class="pre">outputs</span></code> and <code class="docutils literal notranslate"><span class="pre">targets</span></code> tensors (default = 1)</p></li>
<li><p><strong>mode</strong> – class summation strategy. Must be one of [‘micro’, ‘macro’,
‘weighted’]. If mode=’micro’, classes are ignored, and metric
are calculated generally. If mode=’macro’, metric are
calculated per-class and than are averaged over all classes.
If mode=’weighted’, metric are calculated per-class and than
summed over all classes with weights.</p></li>
<li><p><strong>weights</strong> – class weights(for mode=”weighted”)</p></li>
<li><p><strong>eps</strong> – epsilon to avoid zero division</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="focallossbinary">
<h3>FocalLossBinary<a class="headerlink" href="#focallossbinary" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.focal.FocalLossBinary">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.focal.</code><code class="sig-name descname">FocalLossBinary</code><span class="sig-paren">(</span><em class="sig-param">ignore: int = None</em>, <em class="sig-param">reduced: bool = False</em>, <em class="sig-param">gamma: float = 2.0</em>, <em class="sig-param">alpha: float = 0.25</em>, <em class="sig-param">threshold: float = 0.5</em>, <em class="sig-param">reduction: str = 'mean'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/focal.html#FocalLossBinary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.focal.FocalLossBinary" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.loss._Loss</span></code></p>
<p>Compute focal loss for binary classification problem.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1708.02002">Focal Loss for Dense Object Detection</a> paper.</p>
<dl class="method">
<dt id="catalyst.contrib.losses.focal.FocalLossBinary.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">ignore: int = None</em>, <em class="sig-param">reduced: bool = False</em>, <em class="sig-param">gamma: float = 2.0</em>, <em class="sig-param">alpha: float = 0.25</em>, <em class="sig-param">threshold: float = 0.5</em>, <em class="sig-param">reduction: str = 'mean'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/focal.html#FocalLossBinary.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.focal.FocalLossBinary.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;TODO: Docs. Contribution is welcome.</p>
</dd></dl>

</dd></dl>

</section>
<section id="focallossmulticlass">
<h3>FocalLossMultiClass<a class="headerlink" href="#focallossmulticlass" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.focal.FocalLossMultiClass">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.focal.</code><code class="sig-name descname">FocalLossMultiClass</code><span class="sig-paren">(</span><em class="sig-param">ignore: int = None</em>, <em class="sig-param">reduced: bool = False</em>, <em class="sig-param">gamma: float = 2.0</em>, <em class="sig-param">alpha: float = 0.25</em>, <em class="sig-param">threshold: float = 0.5</em>, <em class="sig-param">reduction: str = 'mean'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/focal.html#FocalLossMultiClass"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.focal.FocalLossMultiClass" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#catalyst.contrib.losses.focal.FocalLossBinary" title="catalyst.contrib.losses.focal.FocalLossBinary"><code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.losses.focal.FocalLossBinary</span></code></a></p>
<p>Compute focal loss for multiclass problem. Ignores targets having -1 label.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1708.02002">Focal Loss for Dense Object Detection</a> paper.</p>
<dl class="method">
<dt id="catalyst.contrib.losses.focal.FocalLossMultiClass.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">ignore: int = None</em>, <em class="sig-param">reduced: bool = False</em>, <em class="sig-param">gamma: float = 2.0</em>, <em class="sig-param">alpha: float = 0.25</em>, <em class="sig-param">threshold: float = 0.5</em>, <em class="sig-param">reduction: str = 'mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#catalyst.contrib.losses.focal.FocalLossMultiClass.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;TODO: Docs. Contribution is welcome.</p>
</dd></dl>

</dd></dl>

</section>
<section id="focaltrevskyloss">
<h3>FocalTrevskyLoss<a class="headerlink" href="#focaltrevskyloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.trevsky.FocalTrevskyLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.trevsky.</code><code class="sig-name descname">FocalTrevskyLoss</code><span class="sig-paren">(</span><em class="sig-param">alpha: float</em>, <em class="sig-param">beta: Optional[float] = None</em>, <em class="sig-param">gamma: float = 1.3333333333333333</em>, <em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/trevsky.html#FocalTrevskyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.trevsky.FocalTrevskyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The focal trevsky loss.
TrevskyIndex = TP / (TP + alpha * FN + betta * FP)
FocalTrevskyLoss = (1 - TrevskyIndex)^gamma
Node: focal will use per image, so loss will pay more attention on complicated images</p>
<dl class="method">
<dt id="catalyst.contrib.losses.trevsky.FocalTrevskyLoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">alpha: float</em>, <em class="sig-param">beta: Optional[float] = None</em>, <em class="sig-param">gamma: float = 1.3333333333333333</em>, <em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/trevsky.html#FocalTrevskyLoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.trevsky.FocalTrevskyLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – false negative coefficient, bigger alpha bigger penalty for
false negative. Must be in (0, 1)</p></li>
<li><p><strong>beta</strong> – false positive coefficient, bigger alpha bigger penalty for
false positive. Must be in (0, 1), if None beta = (1 - alpha)</p></li>
<li><p><strong>gamma</strong> – focal coefficient. It determines how much the weight of</p></li>
<li><p><strong>examples is reduced.</strong> (<em>simple</em>) – </p></li>
<li><p><strong>class_dim</strong> – indicates class dimention (K) for
<code class="docutils literal notranslate"><span class="pre">outputs</span></code> and <code class="docutils literal notranslate"><span class="pre">targets</span></code> tensors (default = 1)</p></li>
<li><p><strong>mode</strong> – class summation strategy. Must be one of [‘micro’, ‘macro’,
‘weighted’]. If mode=’micro’, classes are ignored, and metric
are calculated generally. If mode=’macro’, metric are
calculated separately and than are averaged over all classes.
If mode=’weighted’, metric are calculated separately and than
summed over all classes with weights.</p></li>
<li><p><strong>weights</strong> – class weights(for mode=”weighted”)</p></li>
<li><p><strong>eps</strong> – epsilon to avoid zero division</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="hingeloss">
<h3>HingeLoss<a class="headerlink" href="#hingeloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.recsys.HingeLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.recsys.</code><code class="sig-name descname">HingeLoss</code><a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#HingeLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.HingeLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.losses.recsys.PairwiseLoss</span></code></p>
<p>Hinge loss function.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib.losses</span> <span class="kn">import</span> <span class="n">recsys</span>

<span class="n">pos_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">neg_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">recsys</span><span class="o">.</span><span class="n">HingeLoss</span><span class="p">()(</span><span class="n">pos_score</span><span class="p">,</span> <span class="n">neg_score</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.recsys.HingeLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">positive_score: torch.Tensor</em>, <em class="sig-param">negative_score: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#HingeLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.HingeLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation method for the hinge loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>positive_score</strong> – Tensor containing predictions for known positive items.</p></li>
<li><p><strong>negative_score</strong> – Tensor containing predictions for sampled negative items.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>computed loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="huberlossv0">
<h3>HuberLossV0<a class="headerlink" href="#huberlossv0" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.regression.HuberLossV0">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.regression.</code><code class="sig-name descname">HuberLossV0</code><span class="sig-paren">(</span><em class="sig-param">clip_delta=1.0</em>, <em class="sig-param">reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/regression.html#HuberLossV0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.regression.HuberLossV0" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>&#64;TODO: Docs. Contribution is welcome.</p>
<dl class="method">
<dt id="catalyst.contrib.losses.regression.HuberLossV0.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">clip_delta=1.0</em>, <em class="sig-param">reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/regression.html#HuberLossV0.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.regression.HuberLossV0.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;TODO: Docs. Contribution is welcome.</p>
</dd></dl>

<dl class="method">
<dt id="catalyst.contrib.losses.regression.HuberLossV0.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">output: torch.Tensor</em>, <em class="sig-param">target: torch.Tensor</em>, <em class="sig-param">weights=None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/regression.html#HuberLossV0.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.regression.HuberLossV0.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;TODO: Docs. Contribution is welcome.</p>
</dd></dl>

</dd></dl>

</section>
<section id="iouloss">
<h3>IoULoss<a class="headerlink" href="#iouloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.iou.IoULoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.iou.</code><code class="sig-name descname">IoULoss</code><span class="sig-paren">(</span><em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/iou.html#IoULoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.iou.IoULoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The intersection over union (Jaccard) loss.
IOULoss = 1 - iou score
iou score = intersection / union = tp / (tp + fp + fn)</p>
<dl class="method">
<dt id="catalyst.contrib.losses.iou.IoULoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/iou.html#IoULoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.iou.IoULoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>class_dim</strong> – indicates class dimention (K) for
<code class="docutils literal notranslate"><span class="pre">outputs</span></code> and <code class="docutils literal notranslate"><span class="pre">targets</span></code> tensors (default = 1)</p></li>
<li><p><strong>mode</strong> – class summation strategy. Must be one of [‘micro’, ‘macro’,
‘weighted’]. If mode=’micro’, classes are ignored, and metric
are calculated generally. If mode=’macro’, metric are
calculated per-class and than are averaged over all classes.
If mode=’weighted’, metric are calculated per-class and than
summed over all classes with weights.</p></li>
<li><p><strong>weights</strong> – class weights(for mode=”weighted”)</p></li>
<li><p><strong>eps</strong> – epsilon to avoid zero division</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="logisticloss">
<h3>LogisticLoss<a class="headerlink" href="#logisticloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.recsys.LogisticLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.recsys.</code><code class="sig-name descname">LogisticLoss</code><a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#LogisticLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.LogisticLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.losses.recsys.PairwiseLoss</span></code></p>
<p>Logistic loss function.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib.losses</span> <span class="kn">import</span> <span class="n">recsys</span>

<span class="n">pos_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">neg_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">recsys</span><span class="o">.</span><span class="n">LogisticLoss</span><span class="p">()(</span><span class="n">pos_score</span><span class="p">,</span> <span class="n">neg_score</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.recsys.LogisticLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">positive_score: torch.Tensor</em>, <em class="sig-param">negative_score: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#LogisticLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.LogisticLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation method for the logistic loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>positive_score</strong> – Tensor containing predictions for known positive items.</p></li>
<li><p><strong>negative_score</strong> – Tensor containing predictions for sampled negative items.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>computed loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="ntxentloss">
<h3>NTXentLoss<a class="headerlink" href="#ntxentloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.ntxent.NTXentLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.ntxent.</code><code class="sig-name descname">NTXentLoss</code><span class="sig-paren">(</span><em class="sig-param">tau: float</em>, <em class="sig-param">reduction: str = 'mean'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/ntxent.html#NTXentLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.ntxent.NTXentLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A Contrastive embedding loss.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/2002.05709">A Simple Framework
for Contrastive Learning of Visual Representations</a>.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib</span> <span class="kn">import</span> <span class="n">NTXentLoss</span>

<span class="n">embeddings_left</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">embeddings_right</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">NTXentLoss</span><span class="p">(</span><span class="n">tau</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">criterion</span><span class="p">(</span><span class="n">embeddings_left</span><span class="p">,</span> <span class="n">embeddings_right</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.ntxent.NTXentLoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">tau: float</em>, <em class="sig-param">reduction: str = 'mean'</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../_modules/catalyst/contrib/losses/ntxent.html#NTXentLoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.ntxent.NTXentLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tau</strong> – temperature</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>.
<code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>: the sum of the output will be divided by the number of
positive pairs in the output,
<code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>: the output will be summed.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if reduction is not mean, sum or none</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="rocstarloss">
<h3>RocStarLoss<a class="headerlink" href="#rocstarloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.recsys.RocStarLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.recsys.</code><code class="sig-name descname">RocStarLoss</code><span class="sig-paren">(</span><em class="sig-param">delta: float = 1.0</em>, <em class="sig-param">sample_size: int = 100</em>, <em class="sig-param">sample_size_gamma: int = 1000</em>, <em class="sig-param">update_gamma_each: int = 50</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#RocStarLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.RocStarLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.losses.recsys.PairwiseLoss</span></code></p>
<p>Roc-star loss function.</p>
<p>Smooth approximation for ROC-AUC. It has been proposed in
<a class="reference external" href="https://github.com/iridiumblue/roc-star">Roc-star: An objective function for ROC-AUC that actually works</a>.</p>
<p>Adapted from:
<a class="reference external" href="https://github.com/iridiumblue/roc-star/issues/2">https://github.com/iridiumblue/roc-star/issues/2</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>delta</strong> – Param from the article. Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><strong>sample_size</strong> – Number of examples to take for ROC AUC approximation. Default: <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>sample_size_gamma</strong> – Number of examples to take for Gamma parameter approximation.
Default: <code class="docutils literal notranslate"><span class="pre">1000</span></code>.</p></li>
<li><p><strong>update_gamma_each</strong> – Number of steps after which to recompute gamma value.
Default: <code class="docutils literal notranslate"><span class="pre">50</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib.losses</span> <span class="kn">import</span> <span class="n">recsys</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">recsys</span><span class="o">.</span><span class="n">RocStarLoss</span><span class="p">()(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.recsys.RocStarLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">outputs: torch.Tensor</em>, <em class="sig-param">targets: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#RocStarLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.RocStarLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation method for the roc-star loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> – Tensor of model predictions in [0, 1] range. Shape <code class="docutils literal notranslate"><span class="pre">(B</span> <span class="pre">x</span> <span class="pre">1)</span></code>.</p></li>
<li><p><strong>targets</strong> – Tensor of true labels in {0, 1}. Shape <code class="docutils literal notranslate"><span class="pre">(B</span> <span class="pre">x</span> <span class="pre">1)</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>computed loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="rsquareloss">
<h3>RSquareLoss<a class="headerlink" href="#rsquareloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.regression.RSquareLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.regression.</code><code class="sig-name descname">RSquareLoss</code><a class="reference internal" href="../_modules/catalyst/contrib/losses/regression.html#RSquareLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.regression.RSquareLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="catalyst.contrib.losses.regression.RSquareLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">outputs: torch.Tensor</em>, <em class="sig-param">targets: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/regression.html#RSquareLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.regression.RSquareLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<em>torch.Tensor</em>) – model outputs</p></li>
<li><p><strong>targets</strong> (<em>torch.Tensor</em>) – targets</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>computed loss</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="supervisedcontrastiveloss">
<h3>SupervisedContrastiveLoss<a class="headerlink" href="#supervisedcontrastiveloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.supervised_contrastive.SupervisedContrastiveLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.supervised_contrastive.</code><code class="sig-name descname">SupervisedContrastiveLoss</code><span class="sig-paren">(</span><em class="sig-param">tau: float</em>, <em class="sig-param">reduction: str = 'mean'</em>, <em class="sig-param">pos_aggregation='in'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/supervised_contrastive.html#SupervisedContrastiveLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.supervised_contrastive.SupervisedContrastiveLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A Contrastive embedding loss that uses targets.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/pdf/2004.11362.pdf">Supervised Contrastive Learning</a>.</p>
<dl class="method">
<dt id="catalyst.contrib.losses.supervised_contrastive.SupervisedContrastiveLoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">tau: float</em>, <em class="sig-param">reduction: str = 'mean'</em>, <em class="sig-param">pos_aggregation='in'</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../_modules/catalyst/contrib/losses/supervised_contrastive.html#SupervisedContrastiveLoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.supervised_contrastive.SupervisedContrastiveLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tau</strong> – temperature</p></li>
<li><p><strong>reduction</strong> – specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>.
<code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>: the sum of the output will be divided by the number of
positive pairs in the output,
<code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>: the output will be summed.</p></li>
<li><p><strong>pos_aggregation</strong> – specifies the place of positive pairs aggregation:
<code class="docutils literal notranslate"><span class="pre">&quot;in&quot;</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;out&quot;</span></code>.
<code class="docutils literal notranslate"><span class="pre">&quot;in&quot;</span></code>: maximization of log(average positive exponentiate similarity)
<code class="docutils literal notranslate"><span class="pre">&quot;out&quot;</span></code>: maximization of average positive similarity</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – if reduction is not mean, sum or none</p></li>
<li><p><strong>ValueError</strong> – if positive aggregation is not in or out</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="trevskyloss">
<h3>TrevskyLoss<a class="headerlink" href="#trevskyloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.trevsky.TrevskyLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.trevsky.</code><code class="sig-name descname">TrevskyLoss</code><span class="sig-paren">(</span><em class="sig-param">alpha: float</em>, <em class="sig-param">beta: Optional[float] = None</em>, <em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/trevsky.html#TrevskyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.trevsky.TrevskyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The trevsky loss.
TrevskyIndex = TP / (TP + alpha * FN + betta * FP)
TrevskyLoss = 1 - TrevskyIndex</p>
<dl class="method">
<dt id="catalyst.contrib.losses.trevsky.TrevskyLoss.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">alpha: float</em>, <em class="sig-param">beta: Optional[float] = None</em>, <em class="sig-param">class_dim: int = 1</em>, <em class="sig-param">mode: str = 'macro'</em>, <em class="sig-param">weights: List[float] = None</em>, <em class="sig-param">eps: float = 1e-07</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/trevsky.html#TrevskyLoss.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.trevsky.TrevskyLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – false negative coefficient, bigger alpha bigger penalty for
false negative. Must be in (0, 1)</p></li>
<li><p><strong>beta</strong> – false positive coefficient, bigger alpha bigger penalty for
false positive. Must be in (0, 1), if None beta = (1 - alpha)</p></li>
<li><p><strong>class_dim</strong> – indicates class dimention (K) for
<code class="docutils literal notranslate"><span class="pre">outputs</span></code> and <code class="docutils literal notranslate"><span class="pre">targets</span></code> tensors (default = 1)</p></li>
<li><p><strong>mode</strong> – class summation strategy. Must be one of [‘micro’, ‘macro’,
‘weighted’]. If mode=’micro’, classes are ignored, and metric
are calculated generally. If mode=’macro’, metric are
calculated separately and than are averaged over all classes.
If mode=’weighted’, metric are calculated separately and than
summed over all classes with weights.</p></li>
<li><p><strong>weights</strong> – class weights(for mode=”weighted”)</p></li>
<li><p><strong>eps</strong> – epsilon to avoid zero division</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="tripletmarginlosswithsampler">
<h3>TripletMarginLossWithSampler<a class="headerlink" href="#tripletmarginlosswithsampler" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.triplet.TripletMarginLossWithSampler">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.triplet.</code><code class="sig-name descname">TripletMarginLossWithSampler</code><span class="sig-paren">(</span><em class="sig-param">margin: float</em>, <em class="sig-param">sampler_inbatch: IInbatchTripletSampler</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/triplet.html#TripletMarginLossWithSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.triplet.TripletMarginLossWithSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>This class combines in-batch sampling of triplets and
default TripletMargingLoss from PyTorch.</p>
<dl class="method">
<dt id="catalyst.contrib.losses.triplet.TripletMarginLossWithSampler.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">margin: float</em>, <em class="sig-param">sampler_inbatch: IInbatchTripletSampler</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/triplet.html#TripletMarginLossWithSampler.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.triplet.TripletMarginLossWithSampler.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> – margin value</p></li>
<li><p><strong>sampler_inbatch</strong> – sampler for forming triplets inside the batch</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="warploss">
<h3>WARPLoss<a class="headerlink" href="#warploss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.losses.recsys.WARPLoss">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.losses.recsys.</code><code class="sig-name descname">WARPLoss</code><span class="sig-paren">(</span><em class="sig-param">max_num_trials: Optional[int] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#WARPLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.WARPLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.losses.recsys.ListwiseLoss</span></code></p>
<p>Weighted Approximate-Rank Pairwise (WARP) loss function.</p>
<p>It has been proposed in <a class="reference external" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37180.pdf">WSABIE: Scaling Up To Large Vocabulary Image Annotation</a> paper.</p>
<p>WARP loss randomly sample output labels of a model, until it finds a pair
which it knows are wrongly labelled and will then only apply an update to
these two incorrectly labelled examples.</p>
<p>Adapted from:
<a class="reference external" href="https://github.com/gabrieltseng/datascience-projects/blob/master/misc/warp.py">https://github.com/gabrieltseng/datascience-projects/blob/master/misc/warp.py</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>max_num_trials</strong> – Number of attempts allowed to find a violating negative example.
In practice it means that we optimize for ranks 1 to max_num_trials-1.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">catalyst.contrib.losses</span> <span class="kn">import</span> <span class="n">recsys</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">recsys</span><span class="o">.</span><span class="n">WARPLoss</span><span class="p">()(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="catalyst.contrib.losses.recsys.WARPLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">outputs: torch.Tensor</em>, <em class="sig-param">targets: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/catalyst/contrib/losses/recsys.html#WARPLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.losses.recsys.WARPLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagation method for the WARP loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> – Iterable of tensors containing predictions for all items.</p></li>
<li><p><strong>targets</strong> – Iterable of tensors containing true labels for all items.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>computed loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="optimizers">
<h2>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">¶</a></h2>
<section id="adamp">
<h3>AdamP<a class="headerlink" href="#adamp" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.optimizers.adamp.AdamP">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.optimizers.adamp.</code><code class="sig-name descname">AdamP</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">betas=(0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">weight_decay=0</em>, <em class="sig-param">delta=0.1</em>, <em class="sig-param">wd_ratio=0.1</em>, <em class="sig-param">nesterov=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/adamp.html#AdamP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.adamp.AdamP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>Implements AdamP algorithm.</p>
<p>The original Adam algorithm was proposed in
<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.
The AdamP variant was proposed in
<a class="reference external" href="https://arxiv.org/abs/2006.08217">Slowing Down the Weight Norm Increase in Momentum-based Optimizers</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – learning rate (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay coefficient
(default: 0)</p></li>
<li><p><strong>delta</strong> – threshold that determines whether
a set of parameters is scale invariant or not (default: 0.1)</p></li>
<li><p><strong>wd_ratio</strong> – relative weight decay applied on scale-invariant
parameters compared to that applied on scale-variant parameters
(default: 0.1)</p></li>
<li><p><strong>nesterov</strong> (<em>boolean</em><em>, </em><em>optional</em>) – enables Nesterov momentum
(default: False)</p></li>
</ul>
</dd>
</dl>
<p>Original source code: <a class="reference external" href="https://github.com/clovaai/AdamP">https://github.com/clovaai/AdamP</a></p>
<dl class="method">
<dt id="catalyst.contrib.optimizers.adamp.AdamP.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">betas=(0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">weight_decay=0</em>, <em class="sig-param">delta=0.1</em>, <em class="sig-param">wd_ratio=0.1</em>, <em class="sig-param">nesterov=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/adamp.html#AdamP.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.adamp.AdamP.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – iterable of parameters to optimize
or dicts defining parameter groups</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – learning rate (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – coefficients
used for computing running averages of gradient
and its square (default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay coefficient
(default: 1e-2)</p></li>
<li><p><strong>delta</strong> – threshold that determines whether
a set of parameters is scale invariant or not (default: 0.1)</p></li>
<li><p><strong>wd_ratio</strong> – relative weight decay applied on scale-invariant
parameters compared to that applied on scale-variant parameters
(default: 0.1)</p></li>
<li><p><strong>nesterov</strong> (<em>boolean</em><em>, </em><em>optional</em>) – enables Nesterov momentum
(default: False)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="lamb">
<h3>Lamb<a class="headerlink" href="#lamb" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.optimizers.lamb.Lamb">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.optimizers.lamb.</code><code class="sig-name descname">Lamb</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr: Optional[float] = 0.001</em>, <em class="sig-param">betas: Optional[Tuple[float</em>, <em class="sig-param">float]] = (0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps: Optional[float] = 1e-06</em>, <em class="sig-param">weight_decay: Optional[float] = 0.0</em>, <em class="sig-param">adam: Optional[bool] = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/lamb.html#Lamb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.lamb.Lamb" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>Implements Lamb algorithm.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1904.00962">Training BERT in 76 minutes</a>.</p>
<dl class="method">
<dt id="catalyst.contrib.optimizers.lamb.Lamb.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr: Optional[float] = 0.001</em>, <em class="sig-param">betas: Optional[Tuple[float</em>, <em class="sig-param">float]] = (0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps: Optional[float] = 1e-06</em>, <em class="sig-param">weight_decay: Optional[float] = 0.0</em>, <em class="sig-param">adam: Optional[bool] = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/lamb.html#Lamb.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.lamb.Lamb.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – iterable of parameters to optimize or dicts
defining parameter groups</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – learning rate (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – coefficients used for
computing running averages of gradient
and its square (default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay (L2 penalty)
(default: 0)</p></li>
<li><p><strong>adam</strong> (<em>bool</em><em>, </em><em>optional</em>) – always use trust ratio = 1, which turns
this into Adam. Useful for comparison purposes.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if invalid learning rate, epsilon value or betas.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="lookahead">
<h3>Lookahead<a class="headerlink" href="#lookahead" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.optimizers.lookahead.Lookahead">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.optimizers.lookahead.</code><code class="sig-name descname">Lookahead</code><span class="sig-paren">(</span><em class="sig-param">optimizer: torch.optim.optimizer.Optimizer</em>, <em class="sig-param">k: int = 5</em>, <em class="sig-param">alpha: float = 0.5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/lookahead.html#Lookahead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.lookahead.Lookahead" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>Implements Lookahead algorithm.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1907.08610">Lookahead Optimizer: k steps forward,
1 step back</a>.</p>
<p>Adapted from:
<a class="reference external" href="https://github.com/alphadl/lookahead.pytorch">https://github.com/alphadl/lookahead.pytorch</a> (MIT License)</p>
<dl class="method">
<dt id="catalyst.contrib.optimizers.lookahead.Lookahead.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">optimizer: torch.optim.optimizer.Optimizer</em>, <em class="sig-param">k: int = 5</em>, <em class="sig-param">alpha: float = 0.5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/lookahead.html#Lookahead.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.lookahead.Lookahead.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;TODO: Docs. Contribution is welcome.</p>
</dd></dl>

</dd></dl>

</section>
<section id="qhadamw">
<h3>QHAdamW<a class="headerlink" href="#qhadamw" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.optimizers.qhadamw.QHAdamW">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.optimizers.qhadamw.</code><code class="sig-name descname">QHAdamW</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">betas=(0.995</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">nus=(0.7</em>, <em class="sig-param">1.0)</em>, <em class="sig-param">weight_decay=0.0</em>, <em class="sig-param">eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/qhadamw.html#QHAdamW"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.qhadamw.QHAdamW" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>Implements QHAdam algorithm.</p>
<p>Combines QHAdam algorithm that was proposed in  <a class="reference external" href="https://arxiv.org/abs/1810.06801">Quasi-hyperbolic momentum
and Adam for deep learning</a> with weight decay decoupling from
<a class="reference external" href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a> paper.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">QHAdamW</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span> <span class="n">nus</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>Adapted from:
<a class="reference external" href="https://github.com/iprally/qhadamw-pytorch/blob/master/qhadamw.py">https://github.com/iprally/qhadamw-pytorch/blob/master/qhadamw.py</a>
(MIT License)</p>
<dl class="method">
<dt id="catalyst.contrib.optimizers.qhadamw.QHAdamW.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">betas=(0.995</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">nus=(0.7</em>, <em class="sig-param">1.0)</em>, <em class="sig-param">weight_decay=0.0</em>, <em class="sig-param">eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/qhadamw.html#QHAdamW.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.qhadamw.QHAdamW.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining parameter
groups</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – learning rate (<span class="math notranslate nohighlight">\(\alpha\)</span> from the paper)
(default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – coefficients used for
computing running averages of the gradient and its square
(default: (0.995, 0.999))</p></li>
<li><p><strong>nus</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – immediate discount factors
used to estimate the gradient and its square
(default: (0.7, 1.0))</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability
(default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay
(L2 regularization coefficient, times two)
(default: 0.0)</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if invalid learning rate, epsilon value, betas or
    weight_decay value.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="radam">
<h3>RAdam<a class="headerlink" href="#radam" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.optimizers.radam.RAdam">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.optimizers.radam.</code><code class="sig-name descname">RAdam</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">betas=(0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">weight_decay=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/radam.html#RAdam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.radam.RAdam" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>Implements RAdam algorithm.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1908.03265">On the Variance of the Adaptive Learning Rate
and Beyond</a>.</p>
<p>&#64;TODO: Docs (add <cite>Example</cite>). Contribution is welcome</p>
<p>Adapted from:
<a class="reference external" href="https://github.com/LiyuanLucasLiu/RAdam">https://github.com/LiyuanLucasLiu/RAdam</a> (Apache-2.0 License)</p>
<dl class="method">
<dt id="catalyst.contrib.optimizers.radam.RAdam.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">betas=(0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">weight_decay=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/radam.html#RAdam.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.radam.RAdam.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;TODO: Docs. Contribution is welcome.</p>
</dd></dl>

</dd></dl>

</section>
<section id="ralamb">
<h3>Ralamb<a class="headerlink" href="#ralamb" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.optimizers.ralamb.Ralamb">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.optimizers.ralamb.</code><code class="sig-name descname">Ralamb</code><span class="sig-paren">(</span><em class="sig-param">params: Iterable</em>, <em class="sig-param">lr: float = 0.001</em>, <em class="sig-param">betas: Tuple[float</em>, <em class="sig-param">float] = (0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps: float = 1e-08</em>, <em class="sig-param">weight_decay: float = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/ralamb.html#Ralamb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.ralamb.Ralamb" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>RAdam optimizer with LARS/LAMB tricks.</p>
<p>Adapted from:
<a class="reference external" href="https://github.com/mgrankin/over9000/blob/master/ralamb.py">https://github.com/mgrankin/over9000/blob/master/ralamb.py</a>
(Apache-2.0 License)</p>
<dl class="method">
<dt id="catalyst.contrib.optimizers.ralamb.Ralamb.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">params: Iterable</em>, <em class="sig-param">lr: float = 0.001</em>, <em class="sig-param">betas: Tuple[float</em>, <em class="sig-param">float] = (0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps: float = 1e-08</em>, <em class="sig-param">weight_decay: float = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/ralamb.html#Ralamb.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.ralamb.Ralamb.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – iterable of parameters to optimize
or dicts defining parameter groups</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – learning rate (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – coefficients used for
computing running averages of gradient
and its square (default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay
(L2 penalty) (default: 0)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="sgdp">
<h3>SGDP<a class="headerlink" href="#sgdp" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.optimizers.sgdp.SGDP">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.optimizers.sgdp.</code><code class="sig-name descname">SGDP</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=&lt;required parameter&gt;</em>, <em class="sig-param">momentum=0</em>, <em class="sig-param">weight_decay=0</em>, <em class="sig-param">dampening=0</em>, <em class="sig-param">nesterov=False</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">delta=0.1</em>, <em class="sig-param">wd_ratio=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/sgdp.html#SGDP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.sgdp.SGDP" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements SGDP algorithm.</p>
<p>The SGDP variant was proposed in
<a class="reference external" href="https://arxiv.org/abs/2006.08217">Slowing Down the Weight Norm Increase in Momentum-based Optimizers</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> – learning rate</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>optional</em>) – momentum factor (default: 0)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
<li><p><strong>dampening</strong> (<em>float</em><em>, </em><em>optional</em>) – dampening for momentum (default: 0)</p></li>
<li><p><strong>nesterov</strong> (<em>bool</em><em>, </em><em>optional</em>) – enables Nesterov momentum (default: False)</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>delta</strong> – threshold that determines whether
a set of parameters is scale invariant or not (default: 0.1)</p></li>
<li><p><strong>wd_ratio</strong> – relative weight decay applied on scale-invariant
parameters compared to that applied on scale-variant parameters
(default: 0.1)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="catalyst.contrib.optimizers.sgdp.SGDP.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=&lt;required parameter&gt;</em>, <em class="sig-param">momentum=0</em>, <em class="sig-param">weight_decay=0</em>, <em class="sig-param">dampening=0</em>, <em class="sig-param">nesterov=False</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">delta=0.1</em>, <em class="sig-param">wd_ratio=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/optimizers/sgdp.html#SGDP.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.optimizers.sgdp.SGDP.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – iterable of parameters to optimize
or dicts defining parameter groups</p></li>
<li><p><strong>lr</strong> – learning rate</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>optional</em>) – momentum factor (default: 0)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay (L2 penalty)
(default: 0)</p></li>
<li><p><strong>dampening</strong> (<em>float</em><em>, </em><em>optional</em>) – dampening for momentum (default: 0)</p></li>
<li><p><strong>nesterov</strong> (<em>bool</em><em>, </em><em>optional</em>) – enables Nesterov momentum
(default: False)</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>delta</strong> – threshold that determines whether
a set of parameters is scale invariant or not (default: 0.1)</p></li>
<li><p><strong>wd_ratio</strong> – relative weight decay applied on scale-invariant
parameters compared to that applied on scale-variant parameters
(default: 0.1)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="schedulers">
<h2>Schedulers<a class="headerlink" href="#schedulers" title="Permalink to this headline">¶</a></h2>
<section id="onecyclelrwithwarmup">
<h3>OneCycleLRWithWarmup<a class="headerlink" href="#onecyclelrwithwarmup" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="catalyst.contrib.schedulers.onecycle.OneCycleLRWithWarmup">
<em class="property">class </em><code class="sig-prename descclassname">catalyst.contrib.schedulers.onecycle.</code><code class="sig-name descname">OneCycleLRWithWarmup</code><span class="sig-paren">(</span><em class="sig-param">optimizer: torch.optim.optimizer.Optimizer</em>, <em class="sig-param">num_steps: int</em>, <em class="sig-param">lr_range=(1.0</em>, <em class="sig-param">0.005)</em>, <em class="sig-param">init_lr: float = None</em>, <em class="sig-param">warmup_steps: int = 0</em>, <em class="sig-param">warmup_fraction: float = None</em>, <em class="sig-param">decay_steps: int = 0</em>, <em class="sig-param">decay_fraction: float = None</em>, <em class="sig-param">momentum_range=(0.8</em>, <em class="sig-param">0.99</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">init_momentum: float = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/schedulers/onecycle.html#OneCycleLRWithWarmup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.schedulers.onecycle.OneCycleLRWithWarmup" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">catalyst.contrib.schedulers.base.BatchScheduler</span></code></p>
<p>OneCycle scheduler with warm-up &amp; lr decay stages.</p>
<p>First stage increases lr from <code class="docutils literal notranslate"><span class="pre">init_lr</span></code> to <code class="docutils literal notranslate"><span class="pre">max_lr</span></code>,
and called <code class="docutils literal notranslate"><span class="pre">warmup</span></code>. Also it decreases momentum
from <code class="docutils literal notranslate"><span class="pre">init_momentum</span></code> to <code class="docutils literal notranslate"><span class="pre">min_momentum</span></code>. Takes <code class="docutils literal notranslate"><span class="pre">warmup_steps</span></code> steps</p>
<p>Second is <code class="docutils literal notranslate"><span class="pre">annealing</span></code> stage. Decrease lr from <code class="docutils literal notranslate"><span class="pre">max_lr</span></code> to <code class="docutils literal notranslate"><span class="pre">min_lr</span></code>,
Increase momentum from <code class="docutils literal notranslate"><span class="pre">min_momentum</span></code> to <code class="docutils literal notranslate"><span class="pre">max_momentum</span></code>.</p>
<p>Third, optional, lr decay.</p>
<dl class="method">
<dt id="catalyst.contrib.schedulers.onecycle.OneCycleLRWithWarmup.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">optimizer: torch.optim.optimizer.Optimizer</em>, <em class="sig-param">num_steps: int</em>, <em class="sig-param">lr_range=(1.0</em>, <em class="sig-param">0.005)</em>, <em class="sig-param">init_lr: float = None</em>, <em class="sig-param">warmup_steps: int = 0</em>, <em class="sig-param">warmup_fraction: float = None</em>, <em class="sig-param">decay_steps: int = 0</em>, <em class="sig-param">decay_fraction: float = None</em>, <em class="sig-param">momentum_range=(0.8</em>, <em class="sig-param">0.99</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">init_momentum: float = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/catalyst/contrib/schedulers/onecycle.html#OneCycleLRWithWarmup.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#catalyst.contrib.schedulers.onecycle.OneCycleLRWithWarmup.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> – PyTorch optimizer</p></li>
<li><p><strong>num_steps</strong> – total number of steps</p></li>
<li><p><strong>lr_range</strong> – tuple with two or three elements
(max_lr, min_lr, [final_lr])</p></li>
<li><p><strong>init_lr</strong> (<em>float</em><em>, </em><em>optional</em>) – initial lr</p></li>
<li><p><strong>warmup_steps</strong> – count of steps for warm-up stage</p></li>
<li><p><strong>warmup_fraction</strong> (<em>float</em><em>, </em><em>optional</em>) – fraction in [0; 1) to calculate
number of warmup steps.
Cannot be set together with <code class="docutils literal notranslate"><span class="pre">warmup_steps</span></code></p></li>
<li><p><strong>decay_steps</strong> – count of steps for lr decay stage</p></li>
<li><p><strong>decay_fraction</strong> (<em>float</em><em>, </em><em>optional</em>) – fraction in [0; 1) to calculate
number of decay steps.
Cannot be set together with <code class="docutils literal notranslate"><span class="pre">decay_steps</span></code></p></li>
<li><p><strong>momentum_range</strong> – tuple with two or three elements
(min_momentum, max_momentum, [final_momentum])</p></li>
<li><p><strong>init_momentum</strong> (<em>float</em><em>, </em><em>optional</em>) – initial momentum</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="core.html" class="btn btn-neutral float-right" title="Core" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="callbacks.html" class="btn btn-neutral" title="Callbacks" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  <hr>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Scitator.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="catalyst-content-right" id="catalyst-content-right">
          <div class="catalyst-right-menu" id="catalyst-right-menu">
            <div class="catalyst-side-scroll" id="catalyst-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Contrib</a><ul>
<li><a class="reference internal" href="#module-catalyst.contrib.data">Data</a><ul>
<li><a class="reference internal" href="#inbatchsamplers">InBatchSamplers</a><ul>
<li><a class="reference internal" href="#inbatchtripletssampler">InBatchTripletsSampler</a></li>
<li><a class="reference internal" href="#alltripletssampler">AllTripletsSampler</a></li>
<li><a class="reference internal" href="#hardtripletssampler">HardTripletsSampler</a></li>
<li><a class="reference internal" href="#hardclustersampler">HardClusterSampler</a></li>
</ul>
</li>
<li><a class="reference internal" href="#samplers">Samplers</a><ul>
<li><a class="reference internal" href="#balancebatchsampler">BalanceBatchSampler</a></li>
<li><a class="reference internal" href="#dynamicbalanceclasssampler">DynamicBalanceClassSampler</a></li>
</ul>
</li>
<li><a class="reference internal" href="#transforms">Transforms</a><ul>
<li><a class="reference internal" href="#compose">Compose</a></li>
<li><a class="reference internal" href="#imagetotensor">ImageToTensor</a></li>
<li><a class="reference internal" href="#normalizeimage">NormalizeImage</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-catalyst.contrib.datasets">Datasets</a><ul>
<li><a class="reference internal" href="#cifar10">CIFAR10</a></li>
<li><a class="reference internal" href="#cifar100">CIFAR100</a></li>
<li><a class="reference internal" href="#imagenette">Imagenette</a></li>
<li><a class="reference internal" href="#imagenette160">Imagenette160</a></li>
<li><a class="reference internal" href="#imagenette320">Imagenette320</a></li>
<li><a class="reference internal" href="#imagewang">Imagewang</a></li>
<li><a class="reference internal" href="#imagewang160">Imagewang160</a></li>
<li><a class="reference internal" href="#imagewang320">Imagewang320</a></li>
<li><a class="reference internal" href="#imagewoof">Imagewoof</a></li>
<li><a class="reference internal" href="#imagewoof160">Imagewoof160</a></li>
<li><a class="reference internal" href="#imagewoof320">Imagewoof320</a></li>
<li><a class="reference internal" href="#mnist">MNIST</a></li>
<li><a class="reference internal" href="#movielens">MovieLens</a></li>
</ul>
</li>
<li><a class="reference internal" href="#layers">Layers</a><ul>
<li><a class="reference internal" href="#adacos">AdaCos</a></li>
<li><a class="reference internal" href="#amsoftmax">AMSoftmax</a></li>
<li><a class="reference internal" href="#arcface">ArcFace</a></li>
<li><a class="reference internal" href="#arcmarginproduct">ArcMarginProduct</a></li>
<li><a class="reference internal" href="#cse">cSE</a></li>
<li><a class="reference internal" href="#cosface">CosFace</a></li>
<li><a class="reference internal" href="#curricularface">CurricularFace</a></li>
<li><a class="reference internal" href="#factorizedlinear">FactorizedLinear</a></li>
<li><a class="reference internal" href="#scse">scSE</a></li>
<li><a class="reference internal" href="#module-catalyst.contrib.layers.softmax">SoftMax</a></li>
<li><a class="reference internal" href="#sse">sSE</a></li>
<li><a class="reference internal" href="#subcenterarcface">SubCenterArcFace</a></li>
</ul>
</li>
<li><a class="reference internal" href="#losses">Losses</a><ul>
<li><a class="reference internal" href="#adaptivehingeloss">AdaptiveHingeLoss</a></li>
<li><a class="reference internal" href="#barlowtwinsloss">BarlowTwinsLoss</a></li>
<li><a class="reference internal" href="#bprloss">BPRLoss</a></li>
<li><a class="reference internal" href="#circleloss">CircleLoss</a></li>
<li><a class="reference internal" href="#diceloss">DiceLoss</a></li>
<li><a class="reference internal" href="#focallossbinary">FocalLossBinary</a></li>
<li><a class="reference internal" href="#focallossmulticlass">FocalLossMultiClass</a></li>
<li><a class="reference internal" href="#focaltrevskyloss">FocalTrevskyLoss</a></li>
<li><a class="reference internal" href="#hingeloss">HingeLoss</a></li>
<li><a class="reference internal" href="#huberlossv0">HuberLossV0</a></li>
<li><a class="reference internal" href="#iouloss">IoULoss</a></li>
<li><a class="reference internal" href="#logisticloss">LogisticLoss</a></li>
<li><a class="reference internal" href="#ntxentloss">NTXentLoss</a></li>
<li><a class="reference internal" href="#rocstarloss">RocStarLoss</a></li>
<li><a class="reference internal" href="#rsquareloss">RSquareLoss</a></li>
<li><a class="reference internal" href="#supervisedcontrastiveloss">SupervisedContrastiveLoss</a></li>
<li><a class="reference internal" href="#trevskyloss">TrevskyLoss</a></li>
<li><a class="reference internal" href="#tripletmarginlosswithsampler">TripletMarginLossWithSampler</a></li>
<li><a class="reference internal" href="#warploss">WARPLoss</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optimizers">Optimizers</a><ul>
<li><a class="reference internal" href="#adamp">AdamP</a></li>
<li><a class="reference internal" href="#lamb">Lamb</a></li>
<li><a class="reference internal" href="#lookahead">Lookahead</a></li>
<li><a class="reference internal" href="#qhadamw">QHAdamW</a></li>
<li><a class="reference internal" href="#radam">RAdam</a></li>
<li><a class="reference internal" href="#ralamb">Ralamb</a></li>
<li><a class="reference internal" href="#sgdp">SGDP</a></li>
</ul>
</li>
<li><a class="reference internal" href="#schedulers">Schedulers</a><ul>
<li><a class="reference internal" href="#onecyclelrwithwarmup">OneCycleLRWithWarmup</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script type="text/javascript" src="../_static/language_data.js"></script>
         <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <footer class="site-footer" id="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://catalyst-team.com/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/catalyst-team/catalyst#step-by-step-guide">Getting started</a></li>
            <li><a class="nav-dropdown-item" href="https://catalyst-team.github.io/catalyst/getting_started/quickstart.html">Quickstart</a></li>
            <li><a class="nav-dropdown-item" href="https://github.com/catalyst-team/catalyst#minimal-examples">Examples</a></li>
            <li><a class="nav-dropdown-item" href="https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&sk=885b4409aecab505db0a63b06f19dcef">Blogpost</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://catalyst-team.com/#projects">Projects</a></li>
            <li><a class="nav-dropdown-item" href="https://github.com/catalyst-team/catalyst">Catalyst</a></li>
            <li><a class="nav-dropdown-item" href="https://github.com/catalyst-team/codestyle">Codestyle</a></li>
            <li><a class="nav-dropdown-item" href="https://github.com/catalyst-team/dl-course">Course</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://catalyst-team.com/">Support</a></li>
            <li><a href="https://github.com/catalyst-team/catalyst#acknowledgments" target="_blank">Acknowledgments</a></li>
            <li><a href="https://github.com/catalyst-team/catalyst/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
            <li><a href="https://opencollective.com/catalyst" target="_blank">Open Collective</a></li>
            <li><a href="https://www.patreon.com/catalyst_team" target="_blank">Patreon</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <div class="footer-social-icons">
            <a href="https://t.me/catalyst_team" target="_blank" class="telegram"></a>
            <a href="https://twitter.com/CatalystTeam" target="_blank" class="twitter"></a>
            <a href="https://join.slack.com/t/catalyst-team-devs/shared_invite/zt-d9miirnn-z86oKDzFMKlMG4fgFdZafw" target="_blank" class="ods"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>


  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://catalyst-team.com/" aria-label="Catalyst"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/catalyst-team/catalyst">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      catalystAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.catalyst-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>